{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from flickrapi import FlickrAPI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(image_tag,MAX_COUNT): -> list\n",
    "    '''\n",
    "    Creates a list of urls to find images with a particular tag on Flickr.\n",
    "    \n",
    "    Args:\n",
    "        str image_tag: The tag to search for see Flickr documentation.\n",
    "        \n",
    "    Returns: \n",
    "        list urls: list of urls of each image\n",
    "    '''\n",
    "    key = ''\n",
    "    secret = ''\n",
    "    flickr = FlickrAPI(key, secret)\n",
    "    photos = flickr.walk(text=image_tag,\n",
    "                            tag_mode='all',\n",
    "                            tags=image_tag,\n",
    "                            extras='url_q',\n",
    "                            per_page=50,\n",
    "                            sort='relevance')\n",
    "    count=0\n",
    "    urls=[]\n",
    "    for photo in photos:\n",
    "        if count< MAX_COUNT:\n",
    "            count += 1\n",
    "            try:\n",
    "                url=photo.get('url_q')\n",
    "                if url == None:\n",
    "                    raise ValueError\n",
    "                urls.append(url)\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            break\n",
    "    print(image_tag+\" done fetching urls, fetched {} urls out of {}\".format(len(urls),MAX_COUNT))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_images(name, urls): -> None\n",
    "    '''\n",
    "    Given the urls, scrapes images off Flickr and puts them in an S3 bucket\n",
    "    \n",
    "    Args:\n",
    "        str name - the root filename for scrapped images\n",
    "        list urls - the location of the images\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    if ' ' in name:\n",
    "         name = name.replace(' ','_').replace(\"\\'\", \"\")\n",
    "    try:       \n",
    "        os.mkdir(name)\n",
    "    except:\n",
    "        pass\n",
    "    for i, url in enumerate(urls):\n",
    "        try:\n",
    "            resp=requests.get(url,stream=True)\n",
    "            filename = name+'/'+name+str(i)\n",
    "            outfile=open(filename, 'wb')\n",
    "            outfile.write(resp.content)\n",
    "            outfile.close()\n",
    "        except Exception as e:\n",
    "            print(i+1, e)\n",
    "    os.system(\"aws s3 mv --recursive \"+name+\" bucket_name\")\n",
    "    os.system(\"rm --recursive -f \"+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COUNT = 2000\n",
    "names = pd.read_csv('missedconnections.csv', header=None)[0]\n",
    "names = names.apply(lambda x: x.strip(' \\xa0\\n'))\n",
    "names = names.values\n",
    "for i, name in enumerate(names):\n",
    "    if ':' not in name:\n",
    "        t0 = time.time()\n",
    "        print(i, end = ' ')\n",
    "        urls = get_urls(name, MAX_COUNT)\n",
    "        put_images(name, urls)\n",
    "        t1 = time.time()\n",
    "        print(name+\" done with upload, job took {} seconds\".format(t1-t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
